{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Modeling metacognitive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rlglue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1f1de0b7e4a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrlglue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rlglue'"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import rlglue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define environment variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In gamble 1, you can win either 6.72 with a probability of 0.86 or win 8.94 with a probability of 0.14.     The expected reward is 7.03.\n",
      "In gamble 2, you can win either -0.94 with a probability of 0.86 or win 7.52 with a probability of 0.14.     The expected reward is 0.24.\n",
      "In gamble 1, you can win either 3.41 with a probability of 0.83 or win -8.28 with a probability of 0.17.     The expected reward is 1.42.\n",
      "In gamble 2, you can win either 9.28 with a probability of 0.83 or win 2.25 with a probability of 0.17.     The expected reward is 8.08.\n",
      "In gamble 1, you can win either 2.7 with a probability of 0.9 or win 1.23 with a probability of 0.1.     The expected reward is 2.55.\n",
      "In gamble 2, you can win either 6.21 with a probability of 0.9 or win 4.19 with a probability of 0.1.     The expected reward is 6.01.\n"
     ]
    }
   ],
   "source": [
    "# TODO: make sure that these are set anew for every new trial while the values are accessible to other functions!\n",
    "\n",
    "def set_env_params():\n",
    "\n",
    "    # probabilities for both gambles\n",
    "    p1 = round(np.random.uniform(0.5, 1), 2)\n",
    "    p2 = round(1 - p1, 2)\n",
    "\n",
    "    # gamble 1\n",
    "    x1 = round(np.random.uniform(-10, 10), 2)\n",
    "    y1 = round(np.random.uniform(-10, 10), 2)\n",
    "\n",
    "    # gamble 2\n",
    "    x2 = round(np.random.uniform(-10, 10), 2)\n",
    "    y2 = round(np.random.uniform(-10, 10), 2)\n",
    "\n",
    "    # expected rewards for each gamble\n",
    "    expected_reward1 = round(x1 * p1 + y1 * p2, 2)\n",
    "    expected_reward2 = round(x2 * p1 + y2 * p2, 2)\n",
    "\n",
    "    # printing out the current trial's parameters\n",
    "    print(f\"In gamble 1, you can win either {x1} with a probability of {p1} or win {y1} with a probability of {p2}. \\\n",
    "    The expected reward is {expected_reward1}.\")\n",
    "    print(f\"In gamble 2, you can win either {x2} with a probability of {p1} or win {y2} with a probability of {p2}. \\\n",
    "    The expected reward is {expected_reward2}.\")\n",
    "    \n",
    "    return x1, x2, y1, y2, p1, p2\n",
    "\n",
    "# test function\n",
    "# for i in range(3):\n",
    "#     set_env_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define gambles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-a6f7b2389013>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# test functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgamble1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mgamble2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-a6f7b2389013>\u001b[0m in \u001b[0;36mgamble1\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgamble1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgamble2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x1' is not defined"
     ]
    }
   ],
   "source": [
    "def gamble1():\n",
    "    result1 = np.random.choice([x1, y1], 1, p=[p1, p2])\n",
    "    return result1\n",
    "\n",
    "def gamble2():\n",
    "    result2 = np.random.choice([x2, y2], 1, p=[p1, p2])\n",
    "    return result2\n",
    "    \n",
    "# test functions\n",
    "gamble1()\n",
    "gamble2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine the gamble with the highest expected value in a given trial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should this function only print the gamble with the highest expected value or also run the gamble function?\n",
    "best_choice = []\n",
    "\n",
    "# accepts two gambles and returns the gamble with the highest expected value\n",
    "def argmax(expected_reward1, expected_reward2):\n",
    "    if expected_reward1 > expected_reward2:\n",
    "        best_choice.append(1) # choose gamble 1\n",
    "        gamble1()\n",
    "    elif expected_reward1 < expected_reward2:\n",
    "        best_choice.append(2) # choose gamble 2\n",
    "        gamble2()\n",
    "    else:\n",
    "        best_choice.append(3) # choose gamble randomly \n",
    "        np.random.choice([gamble1(), gamble2()])(0.5) # correct code to choose randomly between gamble?\n",
    "    return best_choice\n",
    "        \n",
    "# test function\n",
    "# print(argmax(1, 2))\n",
    "# print(argmax(2, 1))\n",
    "# print(argmax(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define agent policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.07]\n"
     ]
    }
   ],
   "source": [
    "# TODO: decide how to define when to use heuristics and how to decide when not using heuristics\n",
    "\n",
    "def policies(p1, p2, expected_reward1, expected_reward2):\n",
    "    # initialize heuristic counters \n",
    "    lex_counter = 0\n",
    "    eqw_counter = 0\n",
    "    \n",
    "    # LEX heuristic\n",
    "    if p1 > p2:             # the \">\" should be \"the larger\" \n",
    "        lex_counter += 1\n",
    "        if x1 > x2:\n",
    "            return gamble1()\n",
    "        elif x2 > x1:\n",
    "            return gamble2()        \n",
    "    # EQW heuristic\n",
    "    elif p1 == p2:          # the \"==\" should be \"almost\" \n",
    "        eqw_counter += 1\n",
    "        # weigh individual outcomes equally\n",
    "        eqw_expected_reward1 = 0.5 * x1 + 0.5 * y1\n",
    "        eqw_expected_reward2 = 0.5 * x2 + 0.5 * y2\n",
    "    \n",
    "        if eqw_expected_reward1 > eqw_expected_reward2:\n",
    "            return gamble1()\n",
    "        elif eqw_expected_reward1 < eqw_expected_reward2:\n",
    "            return gamble2()\n",
    "    # in cases when not using heuristic\n",
    "    else:\n",
    "        if expected_reward1 > expected_reward2:\n",
    "            return gamble1()\n",
    "        elif expected_reward1 < expected_reward2:\n",
    "            return gamble2()    \n",
    "        \n",
    "# test function\n",
    "# print(policies(0.5, 0.5, expected_reward1, expected_reward2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary to keep track of running averages if gambles are identical and only the current trial matters?\n",
    "\n",
    "# intialise a list of observations both both gambles\n",
    "values_gamble1 = []\n",
    "values_gamble2 = []\n",
    "\n",
    "def history(agent_choice, observation):\n",
    "    # add new observations to list\n",
    "    if agent_choice == 'gamble1':\n",
    "        values_gamble1.append(observation)\n",
    "        running_mean_gamble1 = sum(values_gamble1) / len(values_gamble1)\n",
    "        return running_mean_gamble1\n",
    "    elif agent_choice == 'gamble2':\n",
    "        values_gamble2.append(observation)\n",
    "        running_mean_gamble2 = sum(values_gamble2) / len(values_gamble2)\n",
    "        return running_mean_gamble2\n",
    "        \n",
    "# test function\n",
    "# history('gamble2', -5)\n",
    "# history('gamble2', -1)\n",
    "# history('gamble2', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to evaluate agent's overlap with computer choice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define agent and computer choice outside of the function\n",
    "# Only for experimental purposes, no feedback is needed in this case\n",
    "\n",
    "def eval(agent_choice, computer_choice):\n",
    "    counter_correct = 0\n",
    "    if agent_choice == computer_choice:\n",
    "        counter_correct += 1\n",
    "    percentage_correct = counter_correct / num_trials # need to define how to keep track of trial number\n",
    "    return percentage_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'average_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-489-a385433418f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# visualize the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maverage_best\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_runs\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"--\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# define average_best\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_averages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Gamble outcome according to highest expected reward\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Agent using LEX and EQW heuristics\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-489-a385433418f3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# visualize the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maverage_best\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_runs\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"--\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# define average_best\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_averages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Gamble outcome according to highest expected reward\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Agent using LEX and EQW heuristics\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'average_best' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set parameters\n",
    "num_runs = 100\n",
    "\n",
    "agent_scores = np\n",
    "\n",
    "# visualize the result\n",
    "plt.figure(figsize=(15, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot([average_best / num_runs for run in range(num_runs)], linestyle=\"--\") # define average_best\n",
    "plt.plot(np.mean(all_averages, axis=0)) \n",
    "plt.legend([\"Gamble outcome according to highest expected reward\", \"Agent using LEX and EQW heuristics\"])\n",
    "plt.title(\"Average reward of agent\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self)            # what other parameters?\n",
    "    self.actions = [1, 2]         # actions that the agent can take\n",
    "    self.nA = len(self.actions)   # how many actions the agent can take\n",
    "    \n",
    "    def choose_gamble(self, chosen_gamble):\n",
    "    ```\n",
    "    Selects one of the following options:\n",
    "    1: Select gamble 1\n",
    "    2: Select gamble 2\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the agent\n",
    "Agent = Agent():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
